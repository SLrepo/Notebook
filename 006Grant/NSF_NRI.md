[TOC]



# Recent Rewards


## NRI

### NRI: FND: COLLAB: Coordinating Human-Robot Teams in Uncertain Environments (2017)

* PI: Christopher Amato c.amato@northeastern.edu (Principal Investigator)
* Program manager: James Donlon
* Amount: $374,939.00 
* Start date: September 1, 2017
* Northeastern University


* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1734497&HistoricalAwards=false

* Abstract

  The decreasing cost and increasing sophistication of robot hardware is creating new opportunities for teams of robots to be deployed in combination with skilled humans to support and augment labor-intensive and/or dangerous manual work. The vision is for robots to free up time of skilled workers so they can focus on the tasks that they are skilled at (complex problem solving, dextrous manipulation, customer service, etc.) and robots can help with the distracting and frustrating parts of working, such as delivering materials or fetching supplies. This vision is being realized across many sectors of the US economy and abroad, such as in warehouse management, assembly manufacturing, and disaster response. However, progress in this area is being stymied by current methods that are rigid and inflexible, and rely on unrealistic models of human-robot interaction. This project seeks to overcome these problems by proposing new models and methods for teams robots to coordinate with teams humans to complete complex problems.

  In particular, this project will create and solve realistic models for coordinating teams of humans and robots in uncertain environments. The PIs will investigate innovative approaches to this research area, and will make the following contributions: 1) Enable a transformative re-conceptualization of multi-human multi-robot teamwork the accurately reflects the strengths and limitations of the team, as situated within a temporally dynamic, stochastic environment, 2) develop realistic and general models of human-robot teamwork that consider uncertainty and partial observability, and 3) Contribute innovative and scalable techniques for planning and learning in these models. This research will build off of methods that have been successful in single-robot problems under uncertainty and partially observability: partially observable Markov decision processes (POMDPs). POMDPs model robots and environments, but not humans. However, explicitly including people in these models will be critical in almost all real-world applications. By extending POMDPs to multiple robots interacting with teams of humans, complex and realistic problems with mixed human and robot teams can be represented. The solution methods developed in this project will allow the robots to reason about the uncertainty about the domain and their human teammates, while optimizing their behavior. The methods are broadly applicable to human-robot collaboration domains, but they will be evaluated in an emergency department, an environment with a large amount of uncertainty and many delivery and supply tasks during high-volume times. A team of robots can assist in these tasks. Experiments will take place in simulation and in the UC San Diego Simulation and Training Center with various numbers of humans and robots. The results of this project have the potential to transform the way human-robot coordination is performed. 

  ​


###  NRI: Collaborative Research: A Framework for Hierarchical, Probabilistic Planning and Learning

* PI: Marie desJardins mariedj@cs.umbc.edu (Principal Investigator)
* Program manager: Reid Simmons
* Start date: September 1, 2016
* Amount: $373,437.00 
* University of Maryland Baltimore County


* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1637937&HistoricalAwards=false

* Abstract:

  This project is an effort to create a unified framework for solving very large problems with uncertain states and actions, such as manipulator robots acting in real-world environments. The results may have especially great promise for assistive technologies, including autonomous robots that can be used by elderly and disabled populations to aid them in their daily activities. The proposed integrated framework will represent, apply, and learn hierarchical domain knowledge, and will include the ability to transfer knowledge from simpler problems to more complex ones. The research will enable autonomous agents to develop a structured representation of complex domains based on experience. The agents will use learned representations to interpret natural language commands for both low-level and high-level requests.

  The technical focus is enabling tractable planning in large, uncertain domains by generating and leveraging probabilistic domain knowledge at multiple levels of abstraction. Agents will autonomously create layered representations in which the layers build on one another to produce complex behaviors. Agents will learn to perform useful behaviors, such as navigating using low-level sensor feedback or assembling complex objects such as a bridge or a table. The key technical contributions will be methods for (1) planning in large state/action spaces using the abstract object-oriented Markov decision process (AMDP) model, a new formalism for representing probabilistic domain knowledge at multiple levels of abstraction; (2) learning hierarchical task knowledge in the form of AMDPs; and (3) interpreting natural language commands at multiple levels of abstraction by mapping to the learned hierarchical structure. The formalism will be demonstrated and validated in several domains, including a simulated "cleanup" toy domain, challenging and complex video games, and a robot manipulation task. 

* Reference:

  Gopalan, Nakul and desJardins, Marie and Littman, Michael L. and MacGlashan, J. and Squire, S. and Tellex, Stefanie and Winder, John and Wong, Lawson L.. "Planning with Abstract Markov Decision Processes," 27th International Conference on Automated Planning and Scheduling



### NRI: Towards Dexterous Micromanipulation and Assembly

* PI:

  * David Cappelleri dcappell@purdue.edu (Principal Investigator)
  * Karthik Ramani (Co-Principal Investigator)
  * Song Zhang (Co-Principal Investigator)

* PD: Reid Simmons

* Amount: $1,000,000.00 

* Purdue University

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1637961&HistoricalAwards=false

* Abstract:

  Robots that people are familiar with are large, roughly, human-sized. The theory and design tools for developing such macro-scale robots is well developed. By contrast, the theory and design tools applicable to tiny, or micro-scale, robots is nearly non-existent. The goal of this project is to enable the transition of robot manipulation technology from macro-scale robots to micro-scale robots. The expected project outcomes are: i.) controlled and predictable environments for micro-robotic manipulation and assembly; ii.) a new class of 3D vision-based micro-force sensors and a 3D multi-resolution vision system; and iii.) the identification of dexterous micro-manipulation primitives via human micro-teleoperation with new novel haptic probes. These results will enable the assembly of micro-scale systems that are currently not possible. Such systems are applicable across a wide range of domains, such as cm-to-mm scale robots, micro-sensors, steerable catheters, micro-fluidic, and energy harvesting devices.

  At the micro-scale, surface forces dominate the interactions causing unpredictable forces. This project aims to lay the foundations for tools, such as simulators, motion planners, controllers, etc., to be developed for this unique micro-scale environment. The research approach is to reduce the uncertainty in forces present in the micro-world to enable dexterous micro-manipulation and assembly. A new class of manipulation substrates, fixtures, micro-parts, and manipulation tools to control and overcome the levels of adhesion forces present in the micro-world will be created. To enable force control, 3D vision-based micro-force sensing probes along with a multi-resolution 3D vision system to detect the micro-forces in real-time will be developed. Dexterous micro-manipulation primitives will be identified from a human tele-operating a multi-probe micro-manipulation system with micro-force feedback in an augmented reality system. How much and what types of micro-force feedback information is needed for the human to perform different tasks will be studied. These motion primitives together with physics-based simulators can reduce uncertainty in motion planners. The insights gained here will dictate the force-control algorithms implemented in future automated systems. 

* Reference

  Venkatesan, Vinoth and Cappelleri, David J.. "DEVELOPMENT OF AN AUTOMATED FLEXIBLE MICRO-SOLDERING STATION," Proceedings of the ... ASME Design Engineering Technical Conferences, 2017



### NRI: Collaborative Goal and Policy Learning from Human Operators of Construction Co-Robots

* PI: Girish Chowdhary girishc@illinois.edu (Principal Investigator)

* PD: Reid Simmons

* Amount: $703,235.00 

* University of Illinois at Urbana-Champaign

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1719291&HistoricalAwards=false

* Abstract:

  The overall goal of this research is to investigate and significantly advance the science of collaborative interaction between human operators and co-robots. This includes the development of algorithms that can be used to train co-robots from skilled human operators to efficiently perform complex tasks in the face of real-world uncertainty, and to guide novice operators in performing such tasks. The primary targeted application is the construction and farming equipment industry that includes complex co-robots such as excavators, wheel loaders, tractors, forage harvesters where there is a significant need to understand and improve human-robot collaborative learning.

  There are significant scientific challenges in developing efficient algorithms for co-robots that can actively learn from skilled human operators by observing and posing appropriate queries to close the feedback loop between the co-robot and the human operator. This project addresses these challenges by systematically formulating and investigating focused problems to create efficient algorithms that can enhance collaborative human-robot learning. To achieve the goal, algorithms are designed to collaboratively learn latent subgoal structures from ill-defined complex tasks, real-time path planning and control algorithms are developed for co-robots to achieve the learned subgoals, and techniques are developed to provide operator skill specific task decomposition and motion execution guidance. In addition, the developed algorithms are corroborated by simulators, hardware experimentation on laboratory and field co-robots, and theoretical analysis.

* Reference

  * Abramson, Charles I. "A crisis in comparative psychology: where have all the undergraduates gone?," Frontiers in psychology, v.6, 2015.
  * Crick, Christopher and Jay, Graylin and Osentoski, Sarah and Pitzer, Benjamin and Jenkins, Odest Chadwicke. "Rosbridge: Ros for non-ros users," Robotics Research, 2017, p. 493--504.





### NRI: Workers, Firms, and Industries in Robotic Regions

* PI: 
  * Nancey Green Leigh ngleigh@gatech.edu (Principal Investigator)
  * Henrik Christensen (Co-Principal Investigator)


* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1637737&HistoricalAwards=false

* Abstract

  This research project focuses on the U.S. robotics industry and the economic impacts of robotics technology. The researchers will generate new data and then analyze it. The goal of the project is to assess the impacts of robotics on work and the economy following a significant leap in robotic capabilities that has enabled robot-human collaboration. It will build on previous work of the research team, which characterized the U.S. robotics industry and identified regions that host relatively intense research, development, and commerce relating to robotics. The researchers will use a mixed methods approach that situates robot use and diffusion in a regional context. They will survey manufacturers about their robot use and associated employment patterns, and they survey of systems integrators; their prior research has determined that systems integrators are as a group crucial to the U.S. robotics industry. They will also analyze novel labor market data to assess robot-related employment supply and demand, and they will conduct a cross-case comparison of two robotic regions that have significant robotics-related employment. This research will enable current and future policy makers, workers, and corporate leaders to make more informed decisions in anticipation of-and in response to-the diffusion of robots throughout the economy. It will shed light on evolving employment structures, the changing nature of work, firm strategies, and regional economic evolution as robots diffuse. Professional and trade associations, regional planning, workforce and economic development agencies, and popular media will be targeted for dissemination efforts.

  The proposed research will advance understanding of the relationship between 21st century technology and work, meeting a need to assess robots as more than just advanced manufacturing technology, given the rise of collaborative robots in manufacturing and expected diffusion into the service sector. It employs a comprehensive approach to assessing technological change by combining theoretical perspectives and empirical analysis of regional economic development with engineering approaches to robot development and adoption in production. The research team is multidisciplinary, including professors of economic development planning and robotics engineering. The proposed research extends the team's preliminary research, which indicates that distinct robotic regions have emerged within the US. It overcomes the significant deficit of actual data on robot use and diffusion at the national and subnational levels through the use of industry survey and case study methodologies, and through an analysis of real time labor market data made possible through advances in big data collection. By identifying regional factors that lead to innovations in robotics technologies and influence local firm decisions to use robots, this research will equip local policy makers with knowledge to foster competitive and resilient places in the context of rapid technological change.

* Reference

  * Leigh, Nancey Green and Kraft, Benjamin R.. "Emerging robotic regions in the United States: insights for regional economic evolution," Regional Studies, 2017. doi:10.1080/00343404.2016.1269158



### NRI: Collaborative Research: Sketching Geometry and Physics Informed Inference for Mobile Robot Manipulation in Cluttered Scenes

* ​

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1638060&HistoricalAwards=false

* Abstract

  The goal of this project is to improve the ability of robots to manipulate and interact with objects, such as when assisting people to support their daily activities. The key idea is that people can provide robots with important information about their environment and the objects within their environment. In particular, people can use their cognitive skills to name objects, provide an understanding of the geometrical structure of objects, and describe an object's behavior in relation to other objects. Specifically, the project will develop a natural user interface that enables people to provide such information by drawing and sketching on top of the robot's view of the world. Physical simulation will then be used to fill in the missing gaps needed for a robot to complete autonomous manipulation tasks. Thus, the project aims to combine object sketching and physical simulation to better support mobile manipulation tasks as well as learn to perform new manipulation tasks when encountered. The project will support a "Put That There" task, where a user can simply give high-level manipulation commands, with the robot filling in the details necessary to complete the task in a cluttered environment.

  This project aims to improve goal-directed dexterous robotic manipulation in cluttered and unstructured environments through sketching and physical simulation. Robots operating in human environments face considerable uncertainty in perception due to physical contact and occlusions between objects. This project will address such perceptual uncertainty by combining methods for probabilistic inference with natural sketch-based interfaces to extract, label, and automatically infer the geometry, pose, and behavior of objects in complicated scenes. From a human usability perspective, the project addresses how to best create a sketching language and interfaces for intuitive human-in-the-loop extraction of object geometries and behavior from robot sensing. The planned exploration into sketching methods will also explore what underlying representations, raw point clouds, RGB images and video, or RGBD images will be most conducive to supporting accurate geometry extraction and grasp location identification. Given sketched objects, the project will develop probabilistic physically plausible methods for scene estimation that will enable perception for manipulation in cluttered environments. These methods build upon advances in physical simulation to constrain scene estimates to only plausible configurations to both improve estimation accuracy and enable computational tractability. The project will also develop a "Put That There" testbed using a tablet-based web application to support exploration of these concepts as well as act as user studies to evaluate geometry extraction accuracy and the robustness of physics-based scene estimation algorithms.

* Reference

  * Taranta II, Eugene M. and Samiei, Amirreza and Maghoumi, Mehran and Khaloo, Pooya and Pittman, Corey R. and LaViola Jr., Joseph J.. "Jackknife: A Reliable Recognizer with Few Samples and Many Modalities," 2017 CHI Conference on Human Factors in Computing Systems, 2017.



### NRI: Towards Restoring Natural Sensation of Hand Amputees via Wearable Surface Grid Electrodes



* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1637892&HistoricalAwards=false

* Abstract

  Hand amputation can severely limit the quality of life, for example by making it impossible to sense and manipulate objects, or to express gestures. Many robotic prosthetic hands have been developed to date, some of which have dexterity approaching that of a human hand, but a key factor limiting acceptance of these devices is the lack of natural and reliable sensory feedback to the user; the substitution of un-natural stimuli such as skin vibration, visual or audio cues doesn't really cut it. The PI's goal in this research is to explore the use of non-invasive grid electrodes for electrically stimulating the peripheral sensory nerves so they transmit natural (high resolution) haptic sensations to the central nervous system. Success of this project will revolutionize the way in which human beings communicate with robotic prostheses and transform research in close-loop prosthesis control, shifting amputee haptic sensation feedback from invasive implant techniques to non-invasive surface probing techniques. The non-invasive nature of the new technology presents the potential for rapid clinical translations with high functional efficiency and user acceptance. Thus, the research will lead to dramatic improvements in hand amputees' quality of life. In addition, the work will be integrated into graduate and undergraduate student education at the PI's institution, and outreach programs for K-12 students (especially underrepresented STEM students) will expose them to this innovative science.

  This highly creative project adopts an approach that is completely different from the existing techniques for providing sensory restoration/augmentation, and which is supported by the team's preliminary studies. First, the investigators will design a novel, non-invasive nanowire sensor array that will provide natural sensation of the missing hand. The thin-film electrode grid will be self-adhesive and highly stretchable. The multifunctional electrodes will be able not only to provide targeted nerve stimulation but also to record pressures applied on the prosthetic hand, so they can both obtain a rich set of haptic information and also deliver this information to the user accurately and precisely, while inducing minimal interference such as skin discomfort, added weight due to the device, and control signal interference. Second, the team will create a new way of affording sensory restoration by developing a dynamic stimulation scheme that encodes spatially distinct haptic sensations in the digits and palm. This will be achieved by selectively recruiting the various afferent fibers innervating different regions of the hand. The investigators believe that with high spatial resolution based on hand region mapping, the haptic feedback could for the first time enable users to truly perceive the environment by "using" their lost hand, and thereby push the sense of embodiment to a new level. Lastly, new knowledge will be obtained by quantifying the effect of the sensorimotor integration process on closed-loop control of a dexterous prosthetic hand in amputees.

* Reference



### NRI: Goal-Oriented, subject-Adaptive, robot-assisted Locomotor Learning (GOALL)

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1638007&HistoricalAwards=false

* Abstract

  Demand for technology to support gait training after neurological injury is increasing due to population aging. Due to recent advances in sensing, actuation, and computation, robots are ideal tools to deliver gait training, but their potential in gait neurorehabilitation has not yet been fully realized. In this context, crucial difficulties are identified in the employed control schemes, which are required to accommodate inter-individual gait variations, while promoting stable and energetically efficient gait patterns. The proposed project combines experiments with a lower limb exoskeleton with biomechanical modeling to determine subject-specific assistance strategies that enable a new approach to robot-aided gait neurorehabilitation, named GOALL (Goal-Oriented, subject Adaptive, robot-assisted Locomotor Learning). The conducted research activities have relevant applications both in rehabilitation and in human augmentation, while improving our basic understanding of gait biomechanics. The planned education and outreach components will be targeted to engage a community of graduate, undergraduate and K-12 students in topics at the intersection of robotics and biomechanics. The dissemination of the research methods and results in an open source format will benefit the robotics and biomechanics communities.

  The proposed project formalizes new control methods to modulate discrete kinematic variables of gait, achieving controllability of such variables without fully constraining the gait cycle kinematics, thus promoting inter-individual variability in gait kinematics. To this aim, we pursue a systematic approach to the design of gait assistance primitives, i.e. multi-joint coordination patterns capable of modulating a chosen gait parameter, at different gait speeds. The proposed approach is based on inverse dynamics and pulsed torque approximation, and is followed by human-in-the-loop experiments to test the efficacy of assistance primitives to modulate a selected gait parameter during motor adaptation. The experimental investigation is paralleled by neuromechanical modeling of the response to robotic intervention, with the ultimate goal of generalizing the results to other gait parameters of interest for various patient populations. 



### NRI: Collaborative Research: Scalable Robot Autonomy through Remote Operator Assistance and Lifelong Learning



* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1637562&HistoricalAwards=false

* Abstract: 

  One of the most significant barriers to the wider adoption of autonomous robotic systems in commercial applications is the challenge of achieving 100% reliable autonomy in unconstrained human environments. One path toward more robust autonomy is to spend more time in research labs improving robot capabilities, delaying deployment until autonomy is entirely robust. Instead, it may be valuable to deploy robots out in the wild and adapt their behavior based on the rare examples, corner cases, and contingencies encountered after deployment in order to achieve near-term, fully reliable autonomy. This approach is specifically motivated by the call center model, in which robots are deployed at end-user sites and contact a remote human operator for assistance whenever an error is encountered. This project develops a system that enables robots to perform lifelong, incremental improvement from remote human assistance with the long-term goal of achieving full autonomy. This research program has significant broader impacts, making personal robots more accessible to everyday people, while also providing opportunities for human-robot interaction that are ideal for educational K-12 programs, as well as undergraduate and graduate education.

  Towards these goals, novel algorithms, interfaces, and user studies are being developed to advance the state of the art in three key areas related to the call center model: (1) Robust, Multi-Sensory Task Outcome Detection: multimodal techniques for identifying conditions under which to seek assistance or deploy recovery behaviors; (2) Transparency Devices for Situated Awareness: visual and language interface modalities for increasing the situational awareness of the remote operator and allowing for intuitive interaction, leading to more efficient and correct recovery procedures; (3) Low-Level and High-Level Task Model Refinement: lifelong learning techniques for incorporating corrections and recovery procedures into existing task models, as well as active learning methods to collect more targeted data. The proposed approach is being evaluated on a variety of mobile manipulation tasks that a hotel concierge robot might perform, such as delivery tasks or preparing for and cleaning up after a conference banquet. 



### NRI: Collaborative Research: Learning Deep Sensorimotor Policies for Shared Autonomy



* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1748582&HistoricalAwards=false

* Abstract:

  Assistive robots have the potential to transform the lives of persons with upper extremity disabilities, by helping them perform basic daily activities, such as manipulating objects and feeding. However, human control of assistive robots presents substantial challenges. The high dimensionality of robotic arms means that joystick-like interfaces are unnatural hard to use intuitively, and motions resulting from direct teleoperation are often slow, imprecise, and severely limited in their dexterity. This research address these challenges by developing learning algorithms for shared autonomy, where the robot anticipates the user's intent and provides a degree of assistive autonomy to ensure fluid and successful motions. This research will also pave the way for future research that can bootstrap from teleoperation and build towards full robot autonomy.

  The research proposes a hierarchical and multi-phased approach to shared autonomy, using techniques from deep learning and reinforcement learning. The system begins by using deep inverse reinforcement learning to quickly ascertain the user's high-level goal, such as whether the user wants to grasp a particular object or operate an appliance, from raw sensory inputs. This goal inference layer supplies objectives to the lower control layer, which consists of deep neural network control policies that can directly process raw sensory input about the environment and the user to make decisions. These policies choose low-level controls to satisfy the high-level objective while minimizing disagreement with the user's commands. The algorithms will be deployed and tested on a wheelchair-mounted robot arm with the potential to assist users with upper extremity disabilities to perform activities of daily living. 





### NRI: Collaborative Research: Human-Supervised Manipulation of Deformable Objects

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1656101&HistoricalAwards=false

* Abstract:

  The goal of the proposed work is to develop algorithms that enable human-supervised robotic manipulation of deformable objects under significant uncertainty. Manipulation of deformable objects is essential in surgery, which involves complex manipulations of delicate and highly deformable tissue under substantial uncertainty, and in manufacturing, where many assembly tasks involve flexible parts and materials (e.g. cables, textiles, and composites). Recent advances in robot hardware (e.g. the da Vinci and Baxter robots) have made robotic manipulation of deformable objects physically possible but robots still lack the algorithms necessary to perform practical tasks in this domain when there is significant model uncertainty. This project will have broad societal impact through its applications in surgical and manufacturing robotics. The ability to robustly manipulate deformable structures is an important precursor technology towards realizing intelligent robotic surgical assistants. Robotics and its medical applications have the potential to inspire children to pursue careers in STEM fields and meet the needs of America's growing health-care and manufacturing robotics industry. Integration of the research activities with education will emphasize actively involving undergraduates in research activities and introducing new lecture material and projects into undergraduate and graduate courses. Also, special emphasis will be given to recruit qualified students from under-represented groups.

  The purpose of the proposed research is to develop algorithms that enable human-supervised robotic manipulation of deformable objects under substantial uncertainty. Specifically, the research will focus on developing adaptive complexity models for modeling deformable object dynamics and associated uncertainty, planning algorithms for integrated exploration and task execution, control algorithms for robust manipulation of deformable objects under uncertainty, and algorithms for effective human supervision of robotic manipulation of deformable objects. The intellectual merit of the project comes from its fundamental contributions to robotic modeling, planning, and control algorithms for manipulation of deformable objects. The research will have impact in the fields of robotic manipulation, motion planning under uncertainty, and compliant manipulation control. 



###  NRI: INT: COLLAB: Development, Deployment and Evaluation of Personalized Learning Companion Robots for Early Literacy and Language Learning



* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1734443&HistoricalAwards=false

* Abstract

  This National Robotics Initiative project will develop, deploy and evaluate personalized companion robots to assist kindergarten-age children in learning language and vocabulary skills. The aim is to accelerate the impacts of social robots for early education in schools and at home. The four-year project will advance knowledge in three key areas: (1) automatic speech recognition models for young children; (2) multi-modal student assessment algorithms for early language and literacy skills; and (3) personalization of activities, content, and dialogic question generation to boost learning outcomes. The project will generate new insights for how to develop expressive, socially responsive robots that provide more effective, engaging, and empathetic educational experiences for young children. To evaluate the impacts of long-term interactions on educational outcomes, the project team will conduct a 4-month study with Kindergarten classrooms, as well as a 3-month at-home study. The project will engage teachers and parents to develop key guidelines for best practices for use of social robots in classroom and home settings, and participating undergraduate and graduate students will be trained in the multidisciplinary aspects of social robotics, speech recognition and understanding, human participation studies, interactive machine learning for automatic assessment and personalization tools, and early education research.

  This research and development project will be implemented in two phases: An initial phase consisting of short pilot deployments to train and continually iterate development of project technologies and systems, followed by longer term deployment of the robot to examine autonomous interactions with social robots in school and home educational settings. During the development stage of individual components (automatic reading and language assessment tools, automatic question-generation algorithm, automatic speech recognition and spoken language understanding system models, and activities with the autonomous social robot learning companion) the project team will collect and analyze data with practical and performance measures, and refine and iterate each component of the system being developed. After development of the individual components, the autonomous social robot storytelling companion will be developed through repeated iterations with children. In the final year of the project, two 4-month studies will be conducted in six Kindergarten classrooms with 15 to 20 students each. This project is expected to result in five key contributions: (1) Development of Automatic Speech Recognition and Spoken Language Understanding systems for young children's speech, (2) Multi-modal automatic assessment algorithms for Kindergarten age children's spoken language and early reading skills; (3) Automatic personalization algorithms for story content customization and dialogic question generation in the context of young children's verbal storytelling; (4) Development of a fully autonomous, collaborative, peer-like social robot system with effective educational activities; and (5) Long-term studies with deployed social robots in schools and homes spanning several months and demonstrating sustained engagement and positive learning outcomes.


###  NRI: FND: Using Template Models to Identify Exoskeleton User Intent

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1734532&HistoricalAwards=false

* Abstract:

  Exoskeletons have great potential to restore mobility and improve quality of life for individuals with locomotor impairments due to neurotrauma such as spinal cord injury (SCI) and stroke. For several years, clinical exoskeleton use has accelerated gait retraining by enabling patients to take more steps per session, practice more repeatable gait patterns, and have their progress more closely monitored. Today, the technology has matured to the point that multiple systems with FDA approval are commercially available. Still, broader exoskeleton adoption, both in the clinic and for personal use, is needed for the technology to achieve its full potential to impact daily life. The key barrier to such adoption is the lack of a fluent and transparent interface to determine how the user intends to move in conjunction with the exoskeleton. This project seeks to enable exoskeleton use in the diverse scenarios of daily life by developing a robust approach to determine a user's intent. Since the fundamental mechanics of walking are truly universal, the approach is to leverage relatively simple template models of walking that encode these mechanics within algorithms that more reliably identify user intent. Activities of interest include starting from rest, changing walking speed, direction, and cadence, stair ascent/descent, and stopping. The models will be applied to both healthy subjects and individuals with spinal cord injury to identify commonality and critical differences. Beyond the benefits to exoskeleton end-users, the project will facilitate outreach to both K-12 teachers and middle school students to promote education in the STEM disciplines by highlighting how engineering can directly improve quality of life.

  This project aims to improve human machine interface technologies for lower-body exoskeletons by using simple template models of locomotion to more effectively capture user intent. In most existing approaches, mapping sensor data to the user's state/activity is treated as a black-box pattern recognition problem. In contrast, reduced order models or templates are low-dimensional dynamical systems that capture the fundamental mechanics of walking, upon which more complex behaviors play out. The project will investigate physics-based template models to augment inference of exoskeleton user intent for small changes to nominal walking gait (speed, direction, and cadence). Leveraging bio-inspired template control algorithms, extended Kalman filtering and non-parameteric Bayesian approaches will be investigated to solve a stochastic intent observer problem. Next, the work will be extended to detect transitions in user intent for starting, stopping, and gait progression to/from stairs. Throughout, parallel analysis will study customization of the methodology, including the use of entirely different template models, for exoskeleton users with locomotor impairments. Data-driven Floquet analysis will be applied for translation of data from healthy users to augment intent recognition in users with spinal cord injury. Experiments with the new intent recognition will be incorporated into the control system of an FDA-approved exoskeleton that will be used to assess recognition delay and generalization performance of the methods relative to existing techniques. These experiments will be conducted first with healthy subjects and subsequently with individuals with incomplete spinal cord injury. Ultimately, the project has the potential to improve the impact of exoskeletons in areas spanning rehabilitation, search-and-rescue, military, and industrial applications by more effectively capturing user intent. 





### NRI: FND: Collaborative Control for Wearable Robots

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1734399&HistoricalAwards=false

* Abstract:

  There are many applications, including manufacturing, assembly, health care, and construction, where workers may be hampered by not having enough hands to do their job effectively. While one approach is to have a mobile robot assist the human, our project instead focuses on the augmenting human capabilities by developing a wearable robotic arm. Such augmentation of the human body can enhance a person's power, efficiency, safety, and quality of work. The project aims to make these wearable robots act as collaborative teammates, rather than directly controlled passive tools, making them both intuitive for novices and adaptable to expert users. This will significantly improve the efficiency and acceptance of such robots, and positively affect the work conditions of people interacting with them. The robot arm will be tested with human users performing tasks for which a third arm is useful.

  Whereas wearable robotics is a maturing field, there is almost no research on the human-robot interaction (HRI) aspects of such robots. This project investigates collaborative HRI for wearable robots, in four phases: 1) Collecting wearable collaboration data with a human and a tele-operated robot arm; 2) using this data to design an anticipatory Conditional Random Field (CRF) model for the collaboration; 3) developing a Partially Observable Markov Decision Process (POMDP) controller for the robot; and 4) evaluating the controller in human-robot interaction experiments with a physical wearable robotic arm, using metrics of efficiency, fluency, and usability. By doing so, the project contributes to the state-of-the-art in computational HRI by developing new probabilistic models for human-wearable robotic collaboration. The project also contributes new empirical data on how people interact with wearable co-robots through two human-subject studies. The collected data set on human-wearable-robotic interaction will be released to serve the research community. 



###  NRI: INT: SCHooL: Scalable Collaborative Human-Robot Learning

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1734633&HistoricalAwards=false

* Abstract:

  To be useful in warehouses, homes, and other environments from schools to retail stores, robots will need to learn how to robustly manipulate a wide variety of objects. For instance, to enhance the productivity of human workers, service and factory robots could keep specified surfaces clear by identifying, grasping, and relocating objects to appropriate locations. Pre-programming robots to perform such complex manipulation tasks is not feasible; instead this project will investigate scalable robot manipulation, where multiple robots collaboratively learn from multiple humans. The project will contribute new models, algorithms, software, and experimental data to advance the state-of-the-art in deep learning, human-robot interaction, and cloud robotics. To broadly convey the results of this research to students and the public, the project will create a book and video with the Lawrence Hall of Science and the African Robotics Network.

  ​

  Two primary gaps in current understanding of co-robotic Learning from Demonstration (LfD) are: 1) the absence of a theoretical framework that encompasses humans and robots to produce cooperative learning behaviors as optimal solutions; and 2) the lack of research linking LfD with deep learning, hierarchical planning, and human-robot interaction. The project addresses those gaps with a unified theoretical framework based on Inverse Reinforcement Learning and game-theoretic models of communication between humans and robots, treating LfD as a scalable co-robotic process in which multiple humans and multiple networked robots work in a distributed set of environments to maximize a collective set of reward functions and humans learn how to become more effective demonstrators for robots. The research can be applied to almost any context where robots can learn from human demonstrations and will be evaluated in "surface decluttering" benchmarks of increasing complexity over the course of the project. 



### NRI: FND: Human-Robot Collaboration with Distributed and Embodied Intelligence

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1734456&HistoricalAwards=false

* Abstract:

  This award supports research on the issue of intelligence re-embodiment in robots. The fundamental question to be addressed is whether robots should be designed so that different synthetic intelligences can take over. A smart phone can be repurposed to serve different users by swapping chips. Should robots with much higher functionality be designed so that they operate in a suitably similar way? The goal of this project is to answer this question along with a number of related questions including the following. Should robots be designed so different intelligences can take over? Where does the locus of intelligence sit? How does the user understand where it is and who or what is in control? How does re-embodiment impact issues of privacy? To answer these questions, this research effort will adopt a mixed-methods approach including surveys, fieldwork, simulations, and on-site testing of a robot operating system module. The findings of this work are expected to have direct value to robot developers and other researchers. Other impacts include interdisciplinary training of PhD students, and creating research opportunities for undergraduates. Team members plan to use the results of this research to enrich courses on human-robot interactions, and in outreach activities to engage non-academic audiences.

  This research project uses a variety of methods to study the issue of intelligence re-embodiment in robots. Team members will conduct online surveys to systematically assess factors related to re-embodiment. They will engage in fieldwork to investigate current practices of people working and living in environments with several intelligent systems. These findings will in turn be used to inform a user enactment study that will create simulations of real-world contexts and test participants' reactions to different robot behaviors. Team members will also extract and evaluate a set of generalized interaction principles using a second online study. In addition, they will develop a robot operating system (ROS) module to enable re-embodiment of robots by a cloud-based intelligence; the module will be used to assess reactions of business owners and other stakeholders interacting with these systems. The results of these research components will advance knowledge on how people understand re-embodiment. They will serve to map out a set of design recommendations, design features, and the corresponding interaction patterns that do not yet exist around robot behavior in different contexts. They will also lead to the production of software components that support re-embodiment by remote intelligences. More generally, they will help achieve a vision of fully collaborative, ubiquitous, interconnected co-robot systems. 



###  NRI: INT: COLLAB: Robust, Scalable, Distributed Semantic Mapping for Search-and-Rescue and Manufacturing Co-Robots

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1734454&HistoricalAwards=false


* Abstract:

  The goal of this project is to enable multiple co-robots to map and understand the environment they are in to efficiently collaborate among themselves and with human operators in education, medical assistance, agriculture, and manufacturing applications. The first distinctive characteristic of this project is that the environment will be modeled semantically, that is, it will contain human-interpretable labels (e.g., object category names) in addition to geometric data. This will be achieved through a novel, robust integration of methods from both computer vision and robotics, allowing easier communications between robots and humans in the field. The second distinctive characteristic of this project is that the increased computation load due to the addition of human-interpretable information will be handled by judiciously approximating and spreading the computations across the entire network. The novel developed methods will be evaluated by emulating real-world scenarios in manufacturing and for search-and-rescue operations, leading to potential benefits for large segments of the society. The project will include opportunities for training students at the high-school, undergraduate, and graduate levels by promoting the development of marketable skills.

  The project will advance the state of the art in robust semantic mapping from multiple robots by 1) developing a new optimization framework that can handle large, dynamic, uncertain environments under significant measurement errors, 2) explicitly allowing and studying interactions and information exchanges with humans with an hybrid discrete-continuous extension of the optimization framework, and 3) allowing an intelligent use and sharing of the limited computational resources possessed by the network of co-robots as a whole by enabling approximations and balancing of the computations. These developments will be driven by two particular case studies: a job-shop (small factory) scenario, where robots and fixed cameras are used to track and assist human workers during production and assembly of parts; and a classic search-and-rescue scenario, where operators use an heterogeneous team of robots to quickly assess damages and to discover survivors. These two applications, when considered together, highlight all the limitations of the currently prevalent geometric mapping solutions, and will be used as benchmarks for the project's results. 



### NRI: Achieving Selective Kinematics and Stiffness in Flexible Robotics

* Link: https://www.nsf.gov/awardsearch/showAward?AWD_ID=1637838&HistoricalAwards=false

* Abstract: 

  This research project will employ an effect called laminar jamming to create robotic structures that controllably transition between highly compliant and nearly rigid. Precise position control typically requires rigidity, while safe and comfortable interactions with humans requires compliance. The need to incorporate both characteristics arises, for example, in assistive applications such as tremor suppression in Parkinson's disease. It is also necessary for robots working collaboratively with humans in manufacturing settings. Laminar jamming structures are composed of multiple parallel layers that ordinarily slide easily over each other. However, when the laminar structure is squeezed the layers become locked together by friction forces. This project will derive computationally tractable models of laminar structures under external forcing, to enable parametric design and real-time control. Fabrication of laminar jamming devices is simple and inexpensive, lowering barriers to widespread use, both in commercial applications and in educational settings. To ensure dissemination of the results, reference designs, configurations of jamming elements, application prototypes, and testing and performance data will be posted on a popular soft robotics website.

  The combination of laminar jamming and soft robotics opens an entirely new range of robot designs and behaviors. Because the bending stiffness of a beam is proportional to the third power of its thickness, even a few laminae can produce dramatic increases in stiffness when jammed. The goal of this project is to define the capabilities of the technology and derive and validate the fundamental underlying principles. The project consists of three research subtasks: actuator design and testing, computational and analytical modeling, and implementation of key applications. The results of the project will provide a rich set of building blocks for robots that combine the advantageous features of both soft and rigid robots. 



